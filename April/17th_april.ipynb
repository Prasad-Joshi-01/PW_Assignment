{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd777360-a6d9-4668-b5b7-8871de7e3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans:\n",
    "\n",
    "Gradient Boosting Regression (GBR) is a popular machine learning algorithm that is used for regression tasks. \n",
    "It is a type of boosting algorithm that combines multiple weak learners, usually decision trees, to create a strong learner that can make accurate predictions on new data.\n",
    "\n",
    "The GBR algorithm works by iteratively fitting new decision trees to the residual errors of the previous trees,\n",
    "where the residual errors are the differences between the true target values and the predicted values of the previous trees. \n",
    "The goal of each new tree is to reduce the residual errors of the previous trees, which leads to a better overall fit of the model to the data.\n",
    "\n",
    "During each iteration, the GBR algorithm computes the negative gradient of the loss function with respect to the predicted values of the previous trees, \n",
    "which is also called the pseudo-residual. \n",
    "This pseudo-residual is used as the target variable for the new tree, and the new tree is fitted to the data using gradient descent or another optimization algorithm.\n",
    "The predicted values of the new tree are then added to the previous predictions to update the overall prediction of the model.\n",
    "\n",
    "The GBR algorithm also includes several regularization parameters, such as the learning rate, the maximum depth of the trees, and the minimum number of samples required to split a node.\n",
    "These parameters control the complexity of the model and prevent overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53262e-252b-42d8-9cd8-7159f081ed1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51a17d-07f4-494f-9652-09170cdd5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the models\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9821f8a9-0dfc-4561-bcfe-ab3eef5afa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 309116318110.01\n",
      "R-squared: -58344178.78\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, subsample=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.estimators = []\n",
    "        self.intercept = np.mean(y_train)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predicted values to the intercept\n",
    "        y_pred = np.full(len(X), self.intercept)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the negative gradient of the loss function\n",
    "            residuals = y - y_pred\n",
    "            gradient = -residuals\n",
    "            \n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            indices = np.random.choice(len(X), int(self.subsample * len(X)), replace=True)\n",
    "            tree.fit(X[indices], gradient[indices])\n",
    "            \n",
    "            # Add the new tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "            \n",
    "            # Update the predicted values with the new tree's predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        y_pred = np.full(len(X), self.intercept)\n",
    "        \n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        return mse, r2\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.5)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "mse, r2 = gbr.score(X_test, y_test)\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb37719-ef94-454f-bc07-1da85b3998f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a735954-6af5-4a08-aaab-1c1b3c74be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060616cc-cd1c-4d18-a788-ed563534ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "model = GradientBoostingRegressor(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4f2f1-2bdc-40a0-aee2-0f4b949d9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8fd63-0b36-40dc-a192-435317fff7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans:\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple model that has a performance slightly better than random guessing.\n",
    "In Gradient Boosting, the weak learners are decision trees with small depth (also known as \"stumps\"), typically with only one or a few splits.\n",
    "These simple models are combined in an additive manner to form a more complex model that can better fit the training data.\n",
    "\n",
    "The idea behind Gradient Boosting is to sequentially add weak learners to the ensemble, each one trying to correct the errors of the previous learners. \n",
    "The weights of the training samples are adjusted in each iteration to focus on the samples that were misclassified or poorly predicted by the previous models. \n",
    "The final prediction is obtained by summing the predictions of all the weak learners, each one weighted by a factor proportional to its contribution to the overall performance.\n",
    "\n",
    "The use of weak learners has several advantages in Gradient Boosting.\n",
    "First, it makes the algorithm less prone to overfitting, since the individual models are simple and have low variance.\n",
    "Second, it allows the algorithm to capture complex interactions between the features, since the ensemble can combine different decision boundaries to form more complex decision regions. \n",
    "Finally, it makes the algorithm computationally efficient, since each weak learner can be trained independently and in parallel with the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c31aa-d9d9-4279-aab3-bd0be579942e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d5bd5-627a-4334-9490-19512155a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans:\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a powerful predictive model by combining many simple models, each one trying to correct the errors of the previous models.\n",
    "The idea is to iteratively train a sequence of weak learners, such as decision trees, and add them to an ensemble, \n",
    "each one focusing on the samples that were poorly predicted by the previous models.\n",
    "\n",
    "The key idea behind Gradient Boosting is to use gradient descent to optimize the performance of the ensemble with respect to a loss function. \n",
    "In each iteration, the gradient of the loss function with respect to the predictions of the previous models is computed, and a new weak learner is trained to minimize the residual errors. \n",
    "The predictions of the new model are then added to the ensemble, and the weights of the training samples are updated to focus on the samples that were poorly predicted by the previous models.\n",
    "This process is repeated until the performance of the ensemble converges to a minimum.\n",
    "\n",
    "The intuition behind Gradient Boosting can be visualized as follows: imagine you are hiking up a mountain, and you want to reach the summit as quickly as possible.\n",
    "You start at the bottom of the mountain and take a few steps in a certain direction, trying to get as close to the summit as possible. \n",
    "You then evaluate your position and the direction you are facing, and adjust your trajectory to take a step in the direction that will bring you closer to the summit.\n",
    "You repeat this process, taking small steps in the direction of steepest ascent, until you reach the summit.\n",
    "\n",
    "In Gradient Boosting, the loss function represents the distance from the current position to the summit, and the weak learners represent the steps that you take to get closer to the summit. \n",
    "The gradient of the loss function represents the direction of steepest ascent, and the learning rate controls the step size.\n",
    "By iteratively adjusting the trajectory in the direction of steepest ascent,\n",
    "and taking small steps with a controlled learning rate, the Gradient Boosting algorithm can efficiently navigate the landscape of the loss function and find the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0b87d-ac37-4ef4-b0c7-4fc985ebb474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005d116-69a0-4b12-8d42-2404ba914ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans:\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the ensemble, with each model focusing on the residual errors of the previous models.\n",
    "\n",
    "The process starts by initializing the ensemble with a simple model, usually a decision tree with only one split, which makes the best prediction it can based on the available data. \n",
    "This initial model is often called the \"base model\" or the \"bias model\".\n",
    "\n",
    "In the next iteration, a new weak learner is trained to predict the residual errors of the previous model. \n",
    "The residual errors are simply the differences between the actual target values and the predictions of the previous model.\n",
    "The new weak learner is typically a decision tree with small depth, which is trained to minimize the residual errors.\n",
    "The depth of the tree is usually limited to prevent overfitting and to ensure that the individual models are weak.\n",
    "\n",
    "The predictions of the new model are then added to the ensemble, and the process is repeated for a fixed number of iterations, or until the performance of the ensemble converges.\n",
    "At each iteration, the predictions of all the models in the ensemble are summed to obtain the final prediction. \n",
    "The weights of the individual models in the ensemble are usually determined by the performance of the models on the training data, with better-performing models given higher weights.\n",
    "\n",
    "The process of adding new models to the ensemble is often called \"boosting\", since each model is trained to boost the performance of the previous models. \n",
    "The name \"Gradient Boosting\" comes from the fact that the algorithm uses gradient descent to minimize the loss function with respect to the predictions of the ensemble,\n",
    "by iteratively adding new models that minimize the residual errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917b82e-ffb9-4682-b353-956a58566446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14100b-faa2-44f7-8c5e-251770384897",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "Ans:\n",
    "The mathematical intuition behind Gradient Boosting algorithm involves the following steps:\n",
    "1.Define the loss function: The first step is to define a loss function that measures the difference between the predicted values and the true values.\n",
    "The most commonly used loss function for regression problems is the mean squared error (MSE), which is defined as the average of the squared differences between the predicted values and the true values.\n",
    "2.Initialize the ensemble: The second step is to initialize the ensemble with a simple model, usually a decision tree with only one split, which makes the best prediction it can based on the available data. \n",
    "This initial model is often called the \"base model\" or the \"bias model\".\n",
    "3.Compute the residuals: The third step is to compute the residual errors of the base model, which are simply the differences between the actual target values and the predictions of the base model.\n",
    "4.Train a new weak learner: The fourth step is to train a new weak learner, usually a decision tree with small depth, to predict the residual errors of the previous model.\n",
    "The new model is trained to minimize the loss function with respect to the residuals.\n",
    "5.Add the new model to the ensemble: The fifth step is to add the predictions of the new model to the predictions of the previous models in the ensemble, and compute the new predictions.\n",
    "6.Update the residuals: The sixth step is to update the residuals by subtracting the predictions of the new model from the previous residuals, and compute the new residuals.\n",
    "7.Repeat the process: The seventh step is to repeat steps 4 to 6 for a fixed number of iterations, or until the performance of the ensemble converges. \n",
    "At each iteration, a new model is trained to predict the updated residuals, and the predictions of all the models in the ensemble are summed to obtain the final prediction.\n",
    "8.Determine the optimal weights: The eighth step is to determine the optimal weights for the individual models in the ensemble, based on their performance on the training data.\n",
    "The weights are usually determined by minimizing the loss function with respect to the weights, using techniques such as gradient descent.\n",
    "9.Make predictions: The final step is to use the trained ensemble to make predictions on new data.\n",
    "The predictions are obtained by summing the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa2ea7-548e-47a4-8512-52fe3252f0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
