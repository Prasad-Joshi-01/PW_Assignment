{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a6fd5-3667-4370-beec-d112b85b106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "companys health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "Ans:\n",
    "To solve this problem, we can use Bayes theorem, which states:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "Where P(A|B) is the conditional probability of A given B, P(B|A) is the conditional probability of B given A, P(A) is the prior probability of A, and P(B) is the prior probability of B.\n",
    "\n",
    "In this case, we want to find the probability that an employee is a smoker given that he/she uses the health insurance plan. Let's define the events:\n",
    "\n",
    "A = employee is a smoker\n",
    "B = employee uses the health insurance plan\n",
    "\n",
    "Using the information given in the problem, we can find the probabilities:\n",
    "\n",
    "P(B) = 70% = 0.7 (prior probability of using the health insurance plan)\n",
    "P(A|B) = ? (conditional probability of being a smoker given that the employee uses the health insurance plan)\n",
    "P(B|A) = 40% = 0.4 (conditional probability of using the health insurance plan given that the employee is a smoker)\n",
    "P(A) = ? (prior probability of being a smoker)\n",
    "\n",
    "We dont know the prior probability of being a smoker (P(A)), but we can find it using the law of total probability:\n",
    "P(A) = P(A|B) * P(B) + P(A|not B) * P(not B)\n",
    "\n",
    "Where not B means not using the health insurance plan. We know that 70% of the employees use the health insurance plan, so:\n",
    "\n",
    "P(not B) = 30% = 0.3\n",
    "\n",
    "We dont know the conditional probability of being a smoker given that the employee does not use the health insurance plan (P(A|not B)),\n",
    "but we can assume that it is lower than the conditional probability of being a smoker given that the employee uses the health insurance plan (P(A|B)).\n",
    "\n",
    "Lets assume that P(A|not B) = 20% = 0.2. We can now find the prior probability of being a smoker:\n",
    "P(A) = P(A|B) * P(B) + P(A|not B) * P(not B) = 0.4 * 0.7 + 0.2 * 0.3 = 0.34\n",
    "\n",
    "Now we can use Bayes theorem to find the conditional probability of being a smoker given that the employee uses the health insurance plan:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B) = 0.4 * 0.34 / 0.7 = 0.195\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.195, or about 19.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd5933-8673-47b9-bcbf-e81bfdc778a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e086061-2f48-4668-8e95-58abb6d5eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Ans:\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two popular variations of the Naive Bayes algorithm, a probabilistic classification algorithm based on Bayes theorem.\n",
    "\n",
    "The main difference between the two lies in their input feature representation and the assumptions they make about the data.\n",
    "\n",
    "Bernoulli Naive Bayes: It is used when the input features are binary (i.e., each feature can take on only one of two possible values, usually 0 or 1). \n",
    "Bernoulli Naive Bayes assumes that each feature is conditionally independent of the others given the class variable. \n",
    "It is often used in text classification, where the presence or absence of a word is used as a binary feature.\n",
    "\n",
    "Multinomial Naive Bayes: It is used when the input features are counts or frequencies (e.g., the number of times a word appears in a document). \n",
    "Multinomial Naive Bayes assumes that the input features are conditionally independent of each other given the class variable and \n",
    "that the feature counts follow a multinomial distribution.\n",
    "It is also commonly used in text classification, where the frequency of a word is used as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b717467-452b-45e7-80ac-780174941a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514966d8-ed73-425a-8f67-6b84301bbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Ans:\n",
    "Bernoulli Naive Bayes is a classification algorithm that assumes that each feature is conditionally independent of the others given the class variable.\n",
    "In the case of missing values, the algorithm usually treats them as a separate category or ignores them altogether, depending on the specific implementation.\n",
    "\n",
    "One common approach is to treat missing values as a separate category, which can be assigned a specific value such as -1 or NaN. \n",
    "The algorithm then considers this category as a distinct feature and calculates the probabilities accordingly.\n",
    "In this way, the presence or absence of a value can still be used as a feature.\n",
    "\n",
    "Another approach is to simply ignore the missing values and only consider the available features. \n",
    "This can be appropriate if the missing values are rare and do not significantly affect the overall classification accuracy. \n",
    "However, if the missing values are frequent and potentially informative, ignoring them may result in biased or incomplete results.\n",
    "\n",
    "In practice, the choice of how to handle missing values in Bernoulli Naive Bayes depends on the specific application and the characteristics of the data.\n",
    "It is important to carefully evaluate the impact of missing values on the classification performance and select an appropriate approach accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f5a9b-27d8-4586-b24b-092eb9461533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b247f-b13f-40e6-8694-75d3c2900116",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "Ans:\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification, where the goal is to classify instances into one of three or more classes.\n",
    "\n",
    "In Gaussian Naive Bayes, each feature is assumed to follow a Gaussian (normal) distribution, and the algorithm calculates the mean and variance of each feature for each class in the training data. \n",
    "During classification, the algorithm then calculates the probability of the instance belonging to each class based on the likelihood of the features given the class and the prior probability of the class.\n",
    "\n",
    "For multi-class classification, the algorithm can be extended to handle more than two classes by using a \"one-vs-all\" or \"one-vs-one\" strategy.\n",
    "In the \"one-vs-all\" strategy, the algorithm trains a separate binary classifier for each class, which distinguishes that class from all the others.\n",
    "During classification, the algorithm then selects the class with the highest probability score among all the binary classifiers.\n",
    "In the \"one-vs-one\" strategy, the algorithm trains a binary classifier for each pair of classes, \n",
    "and the class with the most votes from all the binary classifiers is selected as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc91359-daff-48cd-b20e-bcd3aaadf72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bd1df-c38e-4c53-9b70-17537d757c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "    \n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db240174-bd83-4e89-8eed-4cf42e5b6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e7835a6-1d41-421c-ae23-de830935cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "df=pd.read_csv(url,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a80fbe8d-1675-4a75-a91d-e937b6193f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4601 non-null   float64\n",
      " 1   1       4601 non-null   float64\n",
      " 2   2       4601 non-null   float64\n",
      " 3   3       4601 non-null   float64\n",
      " 4   4       4601 non-null   float64\n",
      " 5   5       4601 non-null   float64\n",
      " 6   6       4601 non-null   float64\n",
      " 7   7       4601 non-null   float64\n",
      " 8   8       4601 non-null   float64\n",
      " 9   9       4601 non-null   float64\n",
      " 10  10      4601 non-null   float64\n",
      " 11  11      4601 non-null   float64\n",
      " 12  12      4601 non-null   float64\n",
      " 13  13      4601 non-null   float64\n",
      " 14  14      4601 non-null   float64\n",
      " 15  15      4601 non-null   float64\n",
      " 16  16      4601 non-null   float64\n",
      " 17  17      4601 non-null   float64\n",
      " 18  18      4601 non-null   float64\n",
      " 19  19      4601 non-null   float64\n",
      " 20  20      4601 non-null   float64\n",
      " 21  21      4601 non-null   float64\n",
      " 22  22      4601 non-null   float64\n",
      " 23  23      4601 non-null   float64\n",
      " 24  24      4601 non-null   float64\n",
      " 25  25      4601 non-null   float64\n",
      " 26  26      4601 non-null   float64\n",
      " 27  27      4601 non-null   float64\n",
      " 28  28      4601 non-null   float64\n",
      " 29  29      4601 non-null   float64\n",
      " 30  30      4601 non-null   float64\n",
      " 31  31      4601 non-null   float64\n",
      " 32  32      4601 non-null   float64\n",
      " 33  33      4601 non-null   float64\n",
      " 34  34      4601 non-null   float64\n",
      " 35  35      4601 non-null   float64\n",
      " 36  36      4601 non-null   float64\n",
      " 37  37      4601 non-null   float64\n",
      " 38  38      4601 non-null   float64\n",
      " 39  39      4601 non-null   float64\n",
      " 40  40      4601 non-null   float64\n",
      " 41  41      4601 non-null   float64\n",
      " 42  42      4601 non-null   float64\n",
      " 43  43      4601 non-null   float64\n",
      " 44  44      4601 non-null   float64\n",
      " 45  45      4601 non-null   float64\n",
      " 46  46      4601 non-null   float64\n",
      " 47  47      4601 non-null   float64\n",
      " 48  48      4601 non-null   float64\n",
      " 49  49      4601 non-null   float64\n",
      " 50  50      4601 non-null   float64\n",
      " 51  51      4601 non-null   float64\n",
      " 52  52      4601 non-null   float64\n",
      " 53  53      4601 non-null   float64\n",
      " 54  54      4601 non-null   float64\n",
      " 55  55      4601 non-null   int64  \n",
      " 56  56      4601 non-null   int64  \n",
      " 57  57      4601 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ead00ef2-125a-40d0-aa46-c4a514ddcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "X=df.iloc[:,:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "### train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Evaluate the classifiers using 10-fold cross-validation\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "bernoulli_nb_scores = cross_validate(bernoulli_nb, X, y, cv=10, scoring=scoring)\n",
    "multinomial_nb_scores = cross_validate(multinomial_nb, X, y, cv=10, scoring=scoring)\n",
    "gaussian_nb_scores = cross_validate(gaussian_nb, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", bernoulli_nb_scores[\"test_accuracy\"].mean())\n",
    "print(\"Precision:\", bernoulli_nb_scores[\"test_precision\"].mean())\n",
    "print(\"Recall:\", bernoulli_nb_scores[\"test_recall\"].mean())\n",
    "print(\"F1 score:\", bernoulli_nb_scores[\"test_f1\"].mean())\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", multinomial_nb_scores[\"test_accuracy\"].mean())\n",
    "print(\"Precision:\", multinomial_nb_scores[\"test_precision\"].mean())\n",
    "print(\"Recall:\", multinomial_nb_scores[\"test_recall\"].mean())\n",
    "print(\"F1 score:\", multinomial_nb_scores[\"test_f1\"].mean())\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", gaussian_nb_scores[\"test_accuracy\"].mean())\n",
    "print(\"Precision:\", gaussian_nb_scores[\"test_precision\"].mean())\n",
    "print(\"Recall:\", gaussian_nb_scores[\"test_recall\"].mean())\n",
    "print(\"F1 score:\", gaussian_nb_scores[\"test_f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f035c7-4b8f-4a73-ac91-203c4d144163",
   "metadata": {},
   "outputs": [],
   "source": [
    "According to the results, both Bernoulli and Gaussian Naive Bayes classifiers achieved similar performance on this dataset, \n",
    "with slightly higher accuracy for the Bernoulli  variant. \n",
    "Gaussian Naive Bayes higher recall than the other two variants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
